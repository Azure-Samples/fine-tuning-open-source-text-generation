{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation - SamSum \n",
    "\n",
    "This sample shows how use `text-generation` components from the `azureml` system registry to fine tune a model to summarize a dialog between 2 people using samsum dataset. We then deploy the fine tuned model to an online endpoint for real time inference.\n",
    "\n",
    "### Training data\n",
    "We will use the [samsum](https://huggingface.co/datasets/samsum) dataset. This dataset is intended to summarize dialogues between 2 people. with this notebook we will summarize the dialogues and calculate bleu and rouge scores for the summarized text vs provided ground_truth summaries\n",
    "\n",
    "### Outline\n",
    "* Setup pre-requisites such as compute.\n",
    "* Pick a model to fine tune.\n",
    "* Pick and explore training data.\n",
    "* Configure the fine tuning job.\n",
    "* Run the fine tuning job.\n",
    "* Review training and evaluation metrics. \n",
    "* Register the fine tuned model. \n",
    "* Deploy the fine tuned model for real time inference.\n",
    "* Clean up resources. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry\n",
    "* Set an optional experiment name\n",
    "* Check or create compute. A single GPU node can have multiple GPU cards. For example, in one node of `Standard_NC24rs_v3` there are 4 NVIDIA V100 GPUs while in `Standard_NC12s_v3`, there are 2 NVIDIA V100 GPUs. Refer to the [docs](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu) for this information. The number of GPU cards per node is set in the param `gpus_per_node` below. Setting this value correctly will ensure utilization of all GPUs in the node. The recommended GPU compute SKUs can be found [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series) and [here](https://learn.microsoft.com/en-us/azure/virtual-machines/ndv2-series)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies by running below cell. This is not an optional step if running in a new environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-ml in /anaconda/envs/condav0/lib/python3.12/site-packages (1.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (6.0.1)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (0.7.1)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (1.30.2)\n",
      "Requirement already satisfied: azure-mgmt-core>=1.3.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (1.4.0)\n",
      "Requirement already satisfied: marshmallow>=3.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (3.21.3)\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (4.23.0)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (4.66.4)\n",
      "Requirement already satisfied: strictyaml in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (1.7.3)\n",
      "Requirement already satisfied: colorama in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (0.4.6)\n",
      "Requirement already satisfied: pyjwt in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (2.8.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.10.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (12.19.0)\n",
      "Requirement already satisfied: azure-storage-file-share in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (12.17.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake>=12.2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (12.8.0)\n",
      "Requirement already satisfied: pydash>=6.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (8.0.3)\n",
      "Requirement already satisfied: isodate in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (0.6.1)\n",
      "Requirement already satisfied: azure-common>=1.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (1.1.28)\n",
      "Requirement already satisfied: typing-extensions in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (4.12.2)\n",
      "Requirement already satisfied: opencensus-ext-azure in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (1.1.13)\n",
      "Requirement already satisfied: opencensus-ext-logging in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-ai-ml) (0.1.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-core>=1.23.0->azure-ai-ml) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-core>=1.23.0->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-storage-blob>=12.10.0->azure-ai-ml) (43.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/condav0/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.19.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from marshmallow>=3.5->azure-ai-ml) (24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/condav0/lib/python3.12/site-packages (from msrest>=0.6.18->azure-ai-ml) (2024.7.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from msrest>=0.6.18->azure-ai-ml) (2.0.0)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.5.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opencensus-ext-azure->azure-ai-ml) (1.17.1)\n",
      "Requirement already satisfied: opencensus<1.0.0,>=0.11.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opencensus-ext-azure->azure-ai-ml) (0.11.4)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opencensus-ext-azure->azure-ai-ml) (5.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from strictyaml->azure-ai-ml) (2.9.0.post0)\n",
      "Requirement already satisfied: msal>=1.24.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (1.30.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/condav0/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (2.2.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.2)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/condav0/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (2.22)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (5.27.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (1.24.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (2.32.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from msal-extensions>=0.3.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (2.10.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /anaconda/envs/condav0/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: azure-identity in /anaconda/envs/condav0/lib/python3.12/site-packages (1.17.1)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity) (1.30.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity) (43.0.0)\n",
      "Requirement already satisfied: msal>=1.24.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity) (1.30.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-core>=1.23.0->azure-identity) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-core>=1.23.0->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/condav0/lib/python3.12/site-packages (from cryptography>=2.5->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity) (2.8.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from msal-extensions>=0.3.0->azure-identity) (2.10.1)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/condav0/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets==2.9.0\n",
      "  Downloading datasets-2.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.7 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from fsspec[http]>=2021.11.1->datasets==2.9.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (0.25.1)\n",
      "Requirement already satisfied: packaging in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (24.1)\n",
      "Requirement already satisfied: responses<0.19 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from datasets==2.9.0) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from aiohttp->datasets==2.9.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from aiohttp->datasets==2.9.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from aiohttp->datasets==2.9.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from aiohttp->datasets==2.9.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from aiohttp->datasets==2.9.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from aiohttp->datasets==2.9.0) (1.13.1)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/condav0/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.9.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.9.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.9.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.9.0) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from pandas->datasets==2.9.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from pandas->datasets==2.9.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /anaconda/envs/condav0/lib/python3.12/site-packages (from pandas->datasets==2.9.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.9.0) (1.16.0)\n",
      "Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.0.1\n",
      "    Uninstalling datasets-3.0.1:\n",
      "      Successfully uninstalled datasets-3.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-evaluate-mlflow 0.0.61 requires datasets>=2.11.0, but you have datasets 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mlflow in /anaconda/envs/condav0/lib/python3.12/site-packages (2.15.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.15.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (2.15.0)\n",
      "Requirement already satisfied: Flask<4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (1.13.2)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (3.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (3.6)\n",
      "Requirement already satisfied: matplotlib<4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (3.8.4)\n",
      "Requirement already satisfied: numpy<2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (15.0.2)\n",
      "Requirement already satisfied: querystring-parser<2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (1.5.1)\n",
      "Requirement already satisfied: scipy<2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (1.14.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (2.0.31)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<23 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow) (22.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (5.4.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (2.2.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (0.29.0)\n",
      "Requirement already satisfied: entrypoints<1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (7.2.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (1.26.0)\n",
      "Requirement already satisfied: packaging<25 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (24.1)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (5.27.2)\n",
      "Requirement already satisfied: pytz<2025 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (2024.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny==2.15.0->mlflow) (0.5.1)\n",
      "Requirement already satisfied: Mako in /anaconda/envs/condav0/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.2.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from Flask<4->mlflow) (1.8.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in /anaconda/envs/condav0/lib/python3.12/site-packages (from graphene<4->mlflow) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda/envs/condav0/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /anaconda/envs/condav0/lib/python3.12/site-packages (from matplotlib<4->mlflow) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /anaconda/envs/condav0/lib/python3.12/site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /anaconda/envs/condav0/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\n",
      "Requirement already satisfied: six in /anaconda/envs/condav0/lib/python3.12/site-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /anaconda/envs/condav0/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
      "Requirement already satisfied: google-auth~=2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->mlflow) (2.32.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.0->mlflow) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.15.0->mlflow) (3.19.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.0->mlflow) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.0->mlflow) (0.47b0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.0->mlflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.0->mlflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.0->mlflow) (2024.7.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /anaconda/envs/condav0/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.0->mlflow) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.0->mlflow) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->mlflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /anaconda/envs/condav0/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.0->mlflow) (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: azureml-mlflow in /anaconda/envs/condav0/lib/python3.12/site-packages (1.57.0.post1)\n",
      "Requirement already satisfied: jsonpickle in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (3.2.2)\n",
      "Requirement already satisfied: mlflow-skinny in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (2.15.0)\n",
      "Requirement already satisfied: azure-identity in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (1.17.1)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (0.7.1)\n",
      "Requirement already satisfied: azure-core!=1.22.0,<2.0.0,>=1.8.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (1.30.2)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (1.4.0)\n",
      "Requirement already satisfied: azure-storage-blob<=12.19.0,>=12.5.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (12.19.0)\n",
      "Requirement already satisfied: azure-common<2.0.0,>=1.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (1.1.28)\n",
      "Requirement already satisfied: cryptography in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (43.0.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.3 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azureml-mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (4.12.2)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-storage-blob<=12.19.0,>=12.5.0->azureml-mlflow) (0.6.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/condav0/lib/python3.12/site-packages (from cryptography->azureml-mlflow) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/condav0/lib/python3.12/site-packages (from msrest>=0.6.18->azureml-mlflow) (2024.7.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from msrest>=0.6.18->azureml-mlflow) (2.0.0)\n",
      "Requirement already satisfied: msal>=1.24.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity->azureml-mlflow) (1.30.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from azure-identity->azureml-mlflow) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (5.4.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (2.2.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (0.29.0)\n",
      "Requirement already satisfied: entrypoints<1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (7.2.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (1.26.0)\n",
      "Requirement already satisfied: packaging<25 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (24.1)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (5.27.2)\n",
      "Requirement already satisfied: pytz<2025 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (2024.1)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (6.0.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from mlflow-skinny->azureml-mlflow) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/condav0/lib/python3.12/site-packages (from cffi>=1.12->cryptography->azureml-mlflow) (2.22)\n",
      "Requirement already satisfied: google-auth~=2.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny->azureml-mlflow) (2.32.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny->azureml-mlflow) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny->azureml-mlflow) (3.19.2)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity->azureml-mlflow) (2.8.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from msal-extensions>=0.3.0->azure-identity->azureml-mlflow) (2.10.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny->azureml-mlflow) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny->azureml-mlflow) (0.47b0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests>=2.21.0->azure-core!=1.22.0,<2.0.0,>=1.8.0->azureml-mlflow) (2.2.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/condav0/lib/python3.12/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azureml-mlflow) (3.2.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /anaconda/envs/condav0/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny->azureml-mlflow) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny->azureml-mlflow) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny->azureml-mlflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/condav0/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny->azureml-mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /anaconda/envs/condav0/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny->azureml-mlflow) (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "%pip install datasets==2.9.0\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow\n",
    "%pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1727264629439
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "        resource_group_name=os.getenv(\"AZURE_RESOURCE_GROUP\"),\n",
    "        workspace_name=os.getenv(\"AZURE_WORKSPACE_NAME\"),\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "registry_ml_client_msr = MLClient(credential, registry_name=\"azureml-msr\")\n",
    "registry_ml_client_meta = MLClient(credential, registry_name=\"azureml-meta\")\n",
    "experiment_name = \"text-generation-samsum\"\n",
    "\n",
    "# generating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a foundation model to fine tune\n",
    "\n",
    "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites , we need to finetune the model for our specific purpose in order to use it. You can browse these models in the Model Catalog in the AzureML Studio, filtering by the `text-generation` task. In this example, we use the `microsoft-phi-2` model. If you have opened this notebook for a different model, replace the model name and version accordingly. \n",
    "\n",
    "Note the model id property of the model. This will be passed as input to the fine tuning job. This is also available as the `Asset ID` field in model details page in AzureML Studio Model Catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1727261783824
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered Models:\n",
      "Name: Prism, Version: None\n",
      "Name: Virchow2, Version: None\n",
      "Name: Virchow, Version: None\n",
      "Name: BiomedCLIP-PubMedBERT_256-vit_base_patch16_224, Version: None\n",
      "Name: Rouge-Score-Evaluator, Version: None\n",
      "Name: Meteor-Score-Evaluator, Version: None\n",
      "Name: Gleu-Score-Evaluator, Version: None\n",
      "Name: Bleu-Score-Evaluator, Version: None\n",
      "Name: microsoft-llava-med-v1.5-mistral-7b, Version: None\n",
      "Name: MedImageParse, Version: None\n",
      "Name: MedImageInsight, Version: None\n",
      "Name: CxrReportGen, Version: None\n",
      "Name: snowflake-arctic-base, Version: None\n",
      "Name: facebook-dinov2-base-imagenet1k-1-layer, Version: None\n",
      "Name: rai-eval-ui-dag-flow, Version: None\n",
      "Name: Protected-Material-Evaluator, Version: None\n",
      "Name: Indirect-Attack-Evaluator, Version: None\n",
      "Name: ECI-Evaluator, Version: None\n",
      "Name: ALLaM-2-7b-instruct, Version: None\n",
      "Name: rerank-qna, Version: None\n",
      "Name: multi-index-rerank-qna, Version: None\n",
      "Name: Phi-3.5-vision-instruct, Version: None\n",
      "Name: Phi-3.5-mini-instruct, Version: None\n",
      "Name: Phi-3.5-MoE-instruct, Version: None\n",
      "Name: projecte-aina-aguila-7b, Version: None\n",
      "Name: projecte-aina-FLOR-6-3B-Instructed, Version: None\n",
      "Name: projecte-aina-FLOR-6-3B, Version: None\n",
      "Name: projecte-aina-FLOR-1-3B-Instructed, Version: None\n",
      "Name: projecte-aina-FLOR-1-3B, Version: None\n",
      "Name: Phi-3-vision-128k-instruct, Version: None\n",
      "Name: Phi-3-small-8k-instruct, Version: None\n",
      "Name: Phi-3-small-128k-instruct, Version: None\n",
      "Name: Phi-3-medium-4k-instruct, Version: None\n",
      "Name: Phi-3-medium-128k-instruct, Version: None\n",
      "Name: Self-Harm-Related-Content-Evaluator, Version: None\n",
      "Name: Similarity-Evaluator, Version: None\n",
      "Name: Sexual-Content-Evaluator, Version: None\n",
      "Name: Relevance-Evaluator, Version: None\n",
      "Name: Hate-and-Unfairness-Evaluator, Version: None\n",
      "Name: Groundedness-Evaluator, Version: None\n",
      "Name: Fluency-Evaluator, Version: None\n",
      "Name: F1Score-Evaluator, Version: None\n",
      "Name: Violent-Content-Evaluator, Version: None\n",
      "Name: Coherence-Evaluator, Version: None\n",
      "Name: analyze-documents, Version: None\n",
      "Name: analyze-conversations, Version: None\n",
      "Name: rai-qna-quality-safety-eval, Version: None\n",
      "Name: snowflake-arctic-instruct, Version: None\n",
      "Name: Phi-3-mini-4k-instruct, Version: None\n",
      "Name: Phi-3-mini-128k-instruct, Version: None\n",
      "Name: mistralai-Mixtral-8x22B-v0-1, Version: None\n",
      "Name: mistralai-Mixtral-8x22B-Instruct-v0-1, Version: None\n",
      "Name: mistral-community-Mixtral-8x22B-v0-1, Version: None\n",
      "Name: mistralai-Mistral-7B-Instruct-v0-2, Version: None\n",
      "Name: detect-defects, Version: None\n",
      "Name: count-cars, Version: None\n",
      "Name: stabilityai-stable-diffusion-xl-base-1-0, Version: None\n",
      "Name: Facebook-DinoV2-Image-Embeddings-ViT-Giant, Version: None\n",
      "Name: Facebook-DinoV2-Image-Embeddings-ViT-Base, Version: None\n",
      "Name: chat-quality-safety-eval, Version: None\n",
      "Name: qna-quality-safety-eval, Version: None\n",
      "Name: mistralai-Mixtral-8x7B-Instruct-v01, Version: None\n",
      "Name: OpenAI-CLIP-Image-Text-Embeddings-ViT-Large-Patch14-336, Version: None\n",
      "Name: playground-ayod-rag, Version: None\n",
      "Name: deci-decidiffusion-v1-0, Version: None\n",
      "Name: Deci-DeciLM-7B-instruct, Version: None\n",
      "Name: Deci-DeciLM-7B, Version: None\n",
      "Name: Deci-DeciCoder-1b, Version: None\n",
      "Name: mistralai-Mixtral-8x7B-v01, Version: None\n",
      "Name: mmd-3x-yolof_r50_c5_8x8_1x_coco, Version: None\n",
      "Name: mmd-3x-vfnet_x101-64x4d-mdconv-c3-c5_fpn_ms-2x_coco, Version: None\n",
      "Name: mmd-3x-vfnet_r50-mdconv-c3-c5_fpn_ms-2x_coco, Version: None\n",
      "Name: mmd-3x-sparse-rcnn_r50_fpn_300-proposals_crop-ms-480-800-3x_coco, Version: None\n",
      "Name: mmd-3x-sparse-rcnn_r101_fpn_300-proposals_crop-ms-480-800-3x_coco, Version: None\n",
      "Name: mmd-3x-mask-rcnn_swin-t-p4-w7_fpn_1x_coco, Version: None\n",
      "Name: mmd-3x-deformable-detr_refine_twostage_r50_16xb2-50e_coco, Version: None\n",
      "Name: stabilityai-stable-diffusion-xl-refiner-1-0, Version: None\n",
      "Name: AutoML-Image-Object-Detection, Version: None\n",
      "Name: AutoML-Image-Instance-Segmentation, Version: None\n",
      "Name: AutoML-Image-Classification, Version: None\n",
      "Name: mmeft, Version: None\n",
      "Name: ocsort_yolox_x_crowdhuman_mot17-private-half, Version: None\n",
      "Name: bytetrack_yolox_x_crowdhuman_mot17-private-half, Version: None\n",
      "Name: mistralai-Mistral-7B-v01, Version: None\n",
      "Name: mistralai-Mistral-7B-Instruct-v01, Version: None\n",
      "Name: facebook-sam-vit-large, Version: None\n",
      "Name: facebook-sam-vit-huge, Version: None\n",
      "Name: facebook-sam-vit-base, Version: None\n",
      "Name: openai-whisper-large-v3, Version: None\n",
      "Name: qna-rag-metrics-eval, Version: None\n",
      "Name: qna-non-rag-metrics-eval, Version: None\n",
      "Name: Salesforce-BLIP-vqa-base, Version: None\n",
      "Name: Salesforce-BLIP-image-captioning-base, Version: None\n",
      "Name: Salesforce-BLIP-2-opt-2-7b-vqa, Version: None\n",
      "Name: Salesforce-BLIP-2-opt-2-7b-image-to-text, Version: None\n",
      "Name: openai-clip-vit-large-patch14, Version: None\n",
      "Name: OpenAI-CLIP-Image-Text-Embeddings-vit-base-patch32, Version: None\n",
      "Name: openai-clip-vit-base-patch32, Version: None\n",
      "Name: stabilityai-stable-diffusion-2-inpainting, Version: None\n",
      "Name: stabilityai-stable-diffusion-2-1, Version: None\n",
      "Name: runwayml-stable-diffusion-v1-5, Version: None\n",
      "Name: runwayml-stable-diffusion-inpainting, Version: None\n",
      "Name: compvis-stable-diffusion-v1-4, Version: None\n",
      "Name: web-classification, Version: None\n",
      "Name: template-standard-flow, Version: None\n",
      "Name: template-eval-flow, Version: None\n",
      "Name: template-chat-flow, Version: None\n",
      "Name: qna-with-your-own-data-using-faiss-index, Version: None\n",
      "Name: qna-relevance-eval, Version: None\n",
      "Name: qna-groundedness-eval, Version: None\n",
      "Name: qna-gpt-similarity-eval, Version: None\n",
      "Name: qna-fluency-eval, Version: None\n",
      "Name: qna-f1-score-eval, Version: None\n",
      "Name: qna-coherence-eval, Version: None\n",
      "Name: qna-ada-similarity-eval, Version: None\n",
      "Name: how-to-use-functions-with-GPT-chat-API, Version: None\n",
      "Name: classification-accuracy-eval, Version: None\n",
      "Name: chat-with-wikipedia, Version: None\n",
      "Name: bring-your-own-data-chat-qna, Version: None\n",
      "Name: ask-wikipedia, Version: None\n",
      "Name: tiiuae-falcon-40b, Version: None\n",
      "Name: bring-your-own-data-qna, Version: None\n",
      "Name: deformable_detr_twostage_refine_r50_16x2_50e_coco, Version: None\n",
      "Name: sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_480-800_3x_coco, Version: None\n",
      "Name: vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco, Version: None\n",
      "Name: mask_rcnn_swin-t-p4-w7_fpn_1x_coco, Version: None\n",
      "Name: sparse_rcnn_r101_fpn_300_proposals_crop_mstrain_480-800_3x_coco, Version: None\n",
      "Name: yolof_r50_c5_8x8_1x_coco, Version: None\n",
      "Name: microsoft-beit-base-patch16-224-pt22k-ft22k, Version: None\n",
      "Name: google-vit-base-patch16-224, Version: None\n",
      "Name: microsoft-swinv2-base-patch4-window12-192-22k, Version: None\n",
      "Name: vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco, Version: None\n",
      "Name: facebook-deit-base-patch16-224, Version: None\n",
      "Name: tiiuae-falcon-7b, Version: None\n",
      "Name: databricks-dolly-v2-12b, Version: None\n",
      "Name: bert-base-cased, Version: None\n",
      "Name: deepset-minilm-uncased-squad2, Version: None\n",
      "Name: roberta-large-openai-detector, Version: None\n",
      "Name: openai-whisper-large, Version: None\n",
      "Name: sshleifer-distilbart-cnn-12-6, Version: None\n",
      "Name: distilroberta-base, Version: None\n",
      "Name: roberta-base, Version: None\n",
      "Name: finiteautomata-bertweet-base-sentiment-analysis, Version: None\n",
      "Name: facebook-bart-large-cnn, Version: None\n",
      "Name: distilbert-base-cased, Version: None\n",
      "Name: t5-base, Version: None\n",
      "Name: roberta-large, Version: None\n",
      "Name: bert-large-uncased, Version: None\n",
      "Name: deepset-roberta-base-squad2, Version: None\n",
      "Name: distilbert-base-uncased-distilled-squad, Version: None\n",
      "Name: distilbert-base-uncased, Version: None\n",
      "Name: microsoft-deberta-base-mnli, Version: None\n",
      "Name: bert-large-cased, Version: None\n",
      "Name: distilbert-base-uncased-finetuned-sst-2-english, Version: None\n",
      "Name: t5-small, Version: None\n",
      "Name: gpt2-medium, Version: None\n",
      "Name: microsoft-deberta-large-mnli, Version: None\n",
      "Name: gpt2-large, Version: None\n",
      "Name: Jean-Baptiste-camembert-ner, Version: None\n",
      "Name: roberta-large-mnli, Version: None\n",
      "Name: distilbert-base-cased-distilled-squad, Version: None\n",
      "Name: bert-base-uncased, Version: None\n",
      "Name: microsoft-deberta-large, Version: None\n",
      "Name: t5-large, Version: None\n",
      "Name: camembert-base, Version: None\n",
      "Name: microsoft-deberta-xlarge, Version: None\n",
      "Name: roberta-base-openai-detector, Version: None\n",
      "Name: gpt2, Version: None\n",
      "Name: distilgpt2, Version: None\n",
      "Name: microsoft-deberta-base, Version: None\n"
     ]
    }
   ],
   "source": [
    "# List all registered pipeline components\n",
    "models = registry_ml_client.models.list()\n",
    "print(\"Registered Models:\")\n",
    "for model in models:\n",
    "    print(f\"Name: {model.name}, Version: {model.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1727264651076
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using model name: Meta-Llama-3-8B, version: 7, id: azureml://registries/azureml-meta/models/Meta-Llama-3-8B/versions/7 for fine tuning\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Meta-Llama-3-8B\"\n",
    "foundation_model = registry_ml_client_meta.models.get(model_name, label=\"latest\")\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a compute to be used with the job\n",
    "\n",
    "The finetune job works `ONLY` with `GPU` compute. The size of the compute depends on how big the model is and in most cases it becomes tricky to identify the right compute for the job. In this cell, we guide the user to select the right compute for the job.\n",
    "\n",
    "`NOTE1` The computes listed below work with the most optimized configuration. Any changes to the configuration might lead to Cuda Out Of Memory error. In such cases, try to upgrade the compute to a bigger compute size.\n",
    "\n",
    "`NOTE2` While selecting the compute_cluster_size below, make sure the compute is available in your resource group. If a particular compute is not available you can make a request to get access to the compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1727264652885
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computes allow list is not part of model tags\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "if \"computes_allow_list\" in foundation_model.tags:\n",
    "    computes_allow_list = ast.literal_eval(\n",
    "        foundation_model.tags[\"computes_allow_list\"]\n",
    "    )  # convert string to python list\n",
    "    print(f\"Please create a compute from the above list - {computes_allow_list}\")\n",
    "else:\n",
    "    computes_allow_list = None\n",
    "    print(\"Computes allow list is not part of model tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1727264655681
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compute cluster already exists! Reusing it for the current run\n",
      "Number of GPU's in compute Standard_NC48ads_A100_v4: 2\n"
     ]
    }
   ],
   "source": [
    "# If you have a specific compute size to work with change it here. By default we use the 8 x V100 compute from the above list\n",
    "compute_cluster_size = \"Standard_ND40rs_v2\"\n",
    "\n",
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "compute_cluster = \"compute-fine-tuning\"\n",
    "\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster,\n",
    "            size=compute_cluster_size,\n",
    "            tier=\"Dedicated\",\n",
    "            max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(\n",
    "                \"Attempt #2 - Trying to create a low priority compute. Since this is a low priority compute, the job could get pre-empted before completion.\"\n",
    "            )\n",
    "            compute = AmlCompute(\n",
    "                name=compute_cluster,\n",
    "                size=compute_cluster_size,\n",
    "                tier=\"LowPriority\",\n",
    "                max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "            )\n",
    "            workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise ValueError(\n",
    "                f\"WARNING! Compute size {compute_cluster_size} not available in workspace\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Sanity check on the created compute\n",
    "compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "if compute.provisioning_state.lower() == \"failed\":\n",
    "    raise ValueError(\n",
    "        f\"Provisioning failed, Compute '{compute_cluster}' is in failed state. \"\n",
    "        f\"please try creating a different compute\"\n",
    "    )\n",
    "\n",
    "if computes_allow_list is not None:\n",
    "    computes_allow_list_lower_case = [x.lower() for x in computes_allow_list]\n",
    "    if compute.size.lower() not in computes_allow_list_lower_case:\n",
    "        raise ValueError(\n",
    "            f\"VM size {compute.size} is not in the allow-listed computes for finetuning\"\n",
    "        )\n",
    "else:\n",
    "    # Computes with K80 GPUs are not supported\n",
    "    unsupported_gpu_vm_list = [\n",
    "        \"standard_nc6\",\n",
    "        \"standard_nc12\",\n",
    "        \"standard_nc24\",\n",
    "        \"standard_nc24r\",\n",
    "    ]\n",
    "    if compute.size.lower() in unsupported_gpu_vm_list:\n",
    "        raise ValueError(\n",
    "            f\"VM size {compute.size} is currently not supported for finetuning\"\n",
    "        )\n",
    "\n",
    "\n",
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute.\n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpu_count_found = False\n",
    "workspace_compute_sku_list = workspace_ml_client.compute.list_sizes()\n",
    "available_sku_sizes = []\n",
    "for compute_sku in workspace_compute_sku_list:\n",
    "    available_sku_sizes.append(compute_sku.name)\n",
    "    if compute_sku.name.lower() == compute.size.lower():\n",
    "        gpus_per_node = compute_sku.gpus\n",
    "        gpu_count_found = True\n",
    "# if gpu_count_found not found, then print an error\n",
    "if gpu_count_found:\n",
    "    print(f\"Number of GPU's in compute {compute.size}: {gpus_per_node}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Number of GPU's in compute {compute.size} not found. Available skus are: {available_sku_sizes}.\"\n",
    "        f\"This should not happen. Please check the selected compute cluster: {compute_cluster} and try again.\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pick the dataset for fine-tuning the model\n",
    "\n",
    "We use the [samsum](https://huggingface.co/datasets/samsum) dataset. The next few cells show basic data preparation for fine tuning:\n",
    "* Visualize some data rows\n",
    "* Preprocess the data and format it in required format. This is an important step for performing text generation as we add the required sequences/separators in the data. This is how we repurpose the text-generation task to any specific task like summarization, translation, text-completion, etc.\n",
    "* While fintuning, text column is concatenated with ground_truth column to produce finetuning input. Hence, the data should be prepared such that `text + ground_truth` is your actual finetuning data.\n",
    "* bos and eos tokens are added to the data by finetuning pipeline, you do not need to add it explicitly \n",
    "* We want this sample to run quickly, so save smaller `train`, `validation` and `test` files containing 10% of the original. This means the fine tuned model will have lower accuracy, hence it should not be put to real-world use. \n",
    "\n",
    "##### Here is an example of how the data should look like\n",
    "\n",
    "text generation requires the training data to include at least 2 fields – one for ‘text’ and ‘ground_truth’ like in this example. The below examples are from Samsum dataset. \n",
    "\n",
    "Original dataset:\n",
    "\n",
    "| dialogue (text) | summary (ground_truth) |\n",
    "| :- | :- |\n",
    "| Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric: I know! And shows how Americans see Russian ;)\\r\\nRob: And it's really funny!\\r\\nEric: I know! I especially like the train part!\\r\\nRob: Hahaha! No one talks to the machine like that!\\r\\nEric: Is this his only stand-up?\\r\\nRob: Idk. I'll check.\\r\\nEric: Sure.\\r\\nRob: Turns out no! There are some of his stand-ups on youtube.\\r\\nEric: Gr8! I'll watch them now!\\r\\nRob: Me too!\\r\\nEric: MACHINE!\\r\\nRob: MACHINE!\\r\\nEric: TTYL?\\r\\nRob: Sure :) | Eric and Rob are going to watch a stand-up on youtube. | \n",
    "| Will: hey babe, what do you want for dinner tonight?\\r\\nEmma:  gah, don't even worry about it tonight\\r\\nWill: what do you mean? everything ok?\\r\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\r\\nWill: Well what time will you be home?\\r\\nEmma: soon, hopefully\\r\\nWill: you sure? Maybe you want me to pick you up?\\r\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home. \\r\\nWill: Alright, love you. \\r\\nEmma: love you too. | Emma will be home soon and she will let Will know. | \n",
    "\n",
    "Formatted dataset the user might pass:\n",
    "\n",
    "| text (text) | summary (ground_truth) |\n",
    "| :- | :- |\n",
    "| Summarize this dialog:\\nEric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric: I know! And shows how Americans see Russian ;)\\r\\nRob: And it's really funny!\\r\\nEric: I know! I especially like the train part!\\r\\nRob: Hahaha! No one talks to the machine like that!\\r\\nEric: Is this his only stand-up?\\r\\nRob: Idk. I'll check.\\r\\nEric: Sure.\\r\\nRob: Turns out no! There are some of his stand-ups on youtube.\\r\\nEric: Gr8! I'll watch them now!\\r\\nRob: Me too!\\r\\nEric: MACHINE!\\r\\nRob: MACHINE!\\r\\nEric: TTYL?\\r\\nRob: Sure :)\\n---\\nSummary:\\n | Eric and Rob are going to watch a stand-up on youtube. | \n",
    "| Summarize this dialog:\\nWill: hey babe, what do you want for dinner tonight?\\r\\nEmma:  gah, don't even worry about it tonight\\r\\nWill: what do you mean? everything ok?\\r\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\r\\nWill: Well what time will you be home?\\r\\nEmma: soon, hopefully\\r\\nWill: you sure? Maybe you want me to pick you up?\\r\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home. \\r\\nWill: Alright, love you. \\r\\nEmma: love you too. \\n---\\nSummary:\\n | Emma will be home soon and she will let Will know. | \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1727261472032
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████���████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████��████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.94M/2.94M [00:00<00:00, 55.4MB/s]\n",
      "Generating train split: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████���████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████��█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14732/14732 [00:00<00:00, 23970.44 examples/s]\n",
      "Generating test split: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████��████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 5314.26 examples/s]\n",
      "Generating validation split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████��████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 818/818 [00:00<00:00, 5346.33 examples/s]\n",
      "Creating json from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████���████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████��██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 102.62ba/s]\n",
      "Creating json from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████���████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████��████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 197.57ba/s]\n",
      "Creating json from Arrow format: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████���████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████��████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 196.68ba/s]\n"
     ]
    }
   ],
   "source": [
    "# download the dataset using the helper script. This needs datasets library: https://pypi.org/project/datasets/\n",
    "import os\n",
    "\n",
    "exit_status = os.system(\n",
    "    \"python ../../src/core/data_loader/download-dataset.py --download_dir samsum-dataset\"\n",
    ")\n",
    "if exit_status != 0:\n",
    "    raise Exception(\"Error downloading dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1727261472039
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in this election.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\r\\nTim: What did you plan on doing?\\r\\nKim: Oh you know, uni stuff and unfucking my room\\r\\nKim: Maybe tomorrow I'll move my ass and do everything\\r\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\r\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\r\\nTim: It really helps\\r\\nKim: thanks, maybe I'll do that\\r\\nTim: I also like using post-its in kaban style</td>\n",
       "      <td>Kim may try the pomodoro technique recommended by Tim to get more stuff done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella..\\r\\nrachel: Dont say anything else..\\r\\nEdward: What do you mean??\\r\\nrachel: Open your fu**ing door.. I'm outside</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam: hey  overheard rick say something\\r\\nSam: i don't know what to do :-/\\r\\nNaomi: what did he say??\\r\\nSam: he was talking on the phone with someone\\r\\nSam: i don't know who\\r\\nSam: and he was telling them that he wasn't very happy here\\r\\nNaomi: damn!!!\\r\\nSam: he was saying he doesn't like being my roommate\\r\\nNaomi: wow, how do you feel about it?\\r\\nSam: i thought i was a good rommate\\r\\nSam: and that we have a nice place\\r\\nNaomi: that's true man!!!\\r\\nNaomi: i used to love living with you before i moved in with me boyfriend\\r\\nNaomi: i don't know why he's saying that\\r\\nSam: what should i do???\\r\\nNaomi: honestly if it's bothering you that much you should talk to him\\r\\nNaomi: see what's going on\\r\\nSam: i don't want to get in any kind of confrontation though\\r\\nSam: maybe i'll just let it go\\r\\nSam: and see how it goes in the future\\r\\nNaomi: it's your choice sam\\r\\nNaomi: if i were you i would just talk to him and clear the air</td>\n",
       "      <td>Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0  13818513   \n",
       "1  13728867   \n",
       "2  13681000   \n",
       "3  13730747   \n",
       "4  13728094   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  dialogue  \\\n",
       "0  Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1  Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\r\\nTim: What did you plan on doing?\\r\\nKim: Oh you know, uni stuff and unfucking my room\\r\\nKim: Maybe tomorrow I'll move my ass and do everything\\r\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\r\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\r\\nTim: It really helps\\r\\nKim: thanks, maybe I'll do that\\r\\nTim: I also like using post-its in kaban style                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "3  Edward: Rachel, I think I'm in ove with Bella..\\r\\nrachel: Dont say anything else..\\r\\nEdward: What do you mean??\\r\\nrachel: Open your fu**ing door.. I'm outside                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "4  Sam: hey  overheard rick say something\\r\\nSam: i don't know what to do :-/\\r\\nNaomi: what did he say??\\r\\nSam: he was talking on the phone with someone\\r\\nSam: i don't know who\\r\\nSam: and he was telling them that he wasn't very happy here\\r\\nNaomi: damn!!!\\r\\nSam: he was saying he doesn't like being my roommate\\r\\nNaomi: wow, how do you feel about it?\\r\\nSam: i thought i was a good rommate\\r\\nSam: and that we have a nice place\\r\\nNaomi: that's true man!!!\\r\\nNaomi: i used to love living with you before i moved in with me boyfriend\\r\\nNaomi: i don't know why he's saying that\\r\\nSam: what should i do???\\r\\nNaomi: honestly if it's bothering you that much you should talk to him\\r\\nNaomi: see what's going on\\r\\nSam: i don't want to get in any kind of confrontation though\\r\\nSam: maybe i'll just let it go\\r\\nSam: and see how it goes in the future\\r\\nNaomi: it's your choice sam\\r\\nNaomi: if i were you i would just talk to him and clear the air   \n",
       "\n",
       "                                                                                                                                             summary  \n",
       "0  Amanda baked cookies and will bring Jerry some tomorrow.                                                                                           \n",
       "1  Olivia and Olivier are voting for liberals in this election.                                                                                       \n",
       "2  Kim may try the pomodoro technique recommended by Tim to get more stuff done.                                                                      \n",
       "3  Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside.                                                   \n",
       "4  Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the ./samsum-dataset/train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(\"./samsum-dataset/train.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1727261472046
    }
   },
   "outputs": [],
   "source": [
    "# create a function to preprocess the dataset in desired format\n",
    "\n",
    "\n",
    "def get_preprocessed_samsum(df):\n",
    "    prompt = f\"Summarize this dialog:\\n{{}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "    df[\"text\"] = df[\"dialogue\"].map(prompt.format)\n",
    "    df = df.drop(columns=[\"dialogue\", \"id\"])\n",
    "    df = df[[\"text\", \"summary\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1727261472054
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize this dialog:\\nAmanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\\n---\\nSummary:\\n</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize this dialog:\\nOlivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great\\n---\\nSummary:\\n</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in this election.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize this dialog:\\nTim: Hi, what's up?\\r\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\r\\nTim: What did you plan on doing?\\r\\nKim: Oh you know, uni stuff and unfucking my room\\r\\nKim: Maybe tomorrow I'll move my ass and do everything\\r\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\r\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\r\\nTim: It really helps\\r\\nKim: thanks, maybe I'll do that\\r\\nTim: I also like using post-its in kaban style\\n---\\nSummary:\\n</td>\n",
       "      <td>Kim may try the pomodoro technique recommended by Tim to get more stuff done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize this dialog:\\nEdward: Rachel, I think I'm in ove with Bella..\\r\\nrachel: Dont say anything else..\\r\\nEdward: What do you mean??\\r\\nrachel: Open your fu**ing door.. I'm outside\\n---\\nSummary:\\n</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarize this dialog:\\nSam: hey  overheard rick say something\\r\\nSam: i don't know what to do :-/\\r\\nNaomi: what did he say??\\r\\nSam: he was talking on the phone with someone\\r\\nSam: i don't know who\\r\\nSam: and he was telling them that he wasn't very happy here\\r\\nNaomi: damn!!!\\r\\nSam: he was saying he doesn't like being my roommate\\r\\nNaomi: wow, how do you feel about it?\\r\\nSam: i thought i was a good rommate\\r\\nSam: and that we have a nice place\\r\\nNaomi: that's true man!!!\\r\\nNaomi: i used to love living with you before i moved in with me boyfriend\\r\\nNaomi: i don't know why he's saying that\\r\\nSam: what should i do???\\r\\nNaomi: honestly if it's bothering you that much you should talk to him\\r\\nNaomi: see what's going on\\r\\nSam: i don't want to get in any kind of confrontation though\\r\\nSam: maybe i'll just let it go\\r\\nSam: and see how it goes in the future\\r\\nNaomi: it's your choice sam\\r\\nNaomi: if i were you i would just talk to him and clear the air\\n---\\nSummary:\\n</td>\n",
       "      <td>Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "0  Summarize this dialog:\\nAmanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\\n---\\nSummary:\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1  Summarize this dialog:\\nOlivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great\\n---\\nSummary:\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2  Summarize this dialog:\\nTim: Hi, what's up?\\r\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\r\\nTim: What did you plan on doing?\\r\\nKim: Oh you know, uni stuff and unfucking my room\\r\\nKim: Maybe tomorrow I'll move my ass and do everything\\r\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\r\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\r\\nTim: It really helps\\r\\nKim: thanks, maybe I'll do that\\r\\nTim: I also like using post-its in kaban style\\n---\\nSummary:\\n                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "3  Summarize this dialog:\\nEdward: Rachel, I think I'm in ove with Bella..\\r\\nrachel: Dont say anything else..\\r\\nEdward: What do you mean??\\r\\nrachel: Open your fu**ing door.. I'm outside\\n---\\nSummary:\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "4  Summarize this dialog:\\nSam: hey  overheard rick say something\\r\\nSam: i don't know what to do :-/\\r\\nNaomi: what did he say??\\r\\nSam: he was talking on the phone with someone\\r\\nSam: i don't know who\\r\\nSam: and he was telling them that he wasn't very happy here\\r\\nNaomi: damn!!!\\r\\nSam: he was saying he doesn't like being my roommate\\r\\nNaomi: wow, how do you feel about it?\\r\\nSam: i thought i was a good rommate\\r\\nSam: and that we have a nice place\\r\\nNaomi: that's true man!!!\\r\\nNaomi: i used to love living with you before i moved in with me boyfriend\\r\\nNaomi: i don't know why he's saying that\\r\\nSam: what should i do???\\r\\nNaomi: honestly if it's bothering you that much you should talk to him\\r\\nNaomi: see what's going on\\r\\nSam: i don't want to get in any kind of confrontation though\\r\\nSam: maybe i'll just let it go\\r\\nSam: and see how it goes in the future\\r\\nNaomi: it's your choice sam\\r\\nNaomi: if i were you i would just talk to him and clear the air\\n---\\nSummary:\\n   \n",
       "\n",
       "                                                                                                                                             summary  \n",
       "0  Amanda baked cookies and will bring Jerry some tomorrow.                                                                                           \n",
       "1  Olivia and Olivier are voting for liberals in this election.                                                                                       \n",
       "2  Kim may try the pomodoro technique recommended by Tim to get more stuff done.                                                                      \n",
       "3  Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside.                                                   \n",
       "4  Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test.jsonl, train.jsonl and validation.jsonl form the ./samsum-dataset folder into pandas dataframes\n",
    "test_df = pd.read_json(\"./samsum-dataset/test.jsonl\", lines=True)\n",
    "train_df = pd.read_json(\"./samsum-dataset/train.jsonl\", lines=True)\n",
    "validation_df = pd.read_json(\"./samsum-dataset/validation.jsonl\", lines=True)\n",
    "# map the train, validation and test dataframes to preprocess function\n",
    "train_df = get_preprocessed_samsum(train_df)\n",
    "validation_df = get_preprocessed_samsum(validation_df)\n",
    "test_df = get_preprocessed_samsum(test_df)\n",
    "# show the first 5 rows of the train dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10% of the rows from the train, validation and test dataframes into files with small_ prefix in the ./samsum-dataset folder\n",
    "frac = 0.2\n",
    "train_df.sample(frac=frac).to_json(\"./samsum-dataset/small_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1727261472064
    }
   },
   "outputs": [],
   "source": [
    "# save 10% of the rows from the train, validation and test dataframes into files with small_ prefix in the ./samsum-dataset folder\n",
    "frac = 0.8\n",
    "train_df.sample(frac=frac).to_json(\n",
    "    \"./samsum-dataset/small_train.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "validation_df.sample(frac=frac).to_json(\n",
    "    \"./samsum-dataset/small_validation.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "test_df.sample(frac=frac).to_json(\n",
    "    \"./samsum-dataset/small_test.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Submit the fine tuning job using the the model and data as inputs\n",
    " \n",
    "Create the job that uses the `text-generation` pipeline component. [Learn more](https://github.com/Azure/azureml-assets/blob/main/assets/training/finetune_acft_hf_nlp/components/pipeline_components/text_generation/README.md) about all the parameters supported for fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define finetune parameters\n",
    "\n",
    "Finetune parameters can be grouped into 2 categories - training parameters, optimization parameters\n",
    "\n",
    "Training parameters define the training aspects such as - \n",
    "1. the optimizer, scheduler to use\n",
    "2. the metric to optimize the finetune\n",
    "3. number of training steps and the batch size\n",
    "and so on\n",
    "\n",
    "Optimization parameters help in optimizing the GPU memory and effectively using the compute resources. Below are few of the parameters that belong to this category. _The optimization parameters differs for each model and are packaged with the model to handle these variations._\n",
    "1. enable the deepspeed, ORT and LoRA\n",
    "2. enable mixed precision training\n",
    "2. enable multi-node training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "component used \n",
    "\n",
    "https://github.com/Azure/azureml-assets/blob/main/assets/training/finetune_acft_hf_nlp/components/pipeline_components/text_generation/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1727264661216
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following training parameters are enabled - {'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 2, 'learning_rate': 2e-05}\n",
      "The following optimizations are enabled - {'apply_lora': 'true', 'apply_deepspeed': 'true', 'apply_ort': 'true'}\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "training_parameters = dict(\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "print(f\"The following training parameters are enabled - {training_parameters}\")\n",
    "\n",
    "# Optimization parameters - As these parameters are packaged with the model itself, lets retrieve those parameters\n",
    "\n",
    "optimization_parameters = dict(\n",
    "    apply_lora=\"true\", apply_deepspeed=\"true\", apply_ort=\"true\"\n",
    ")\n",
    "print(f\"The following optimizations are enabled - {optimization_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"text_generation_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "    text_generation_pipeline = pipeline_component_func(\n",
    "        mlflow_model_path=foundation_model.id,\n",
    "        compute_model_import=compute_cluster,\n",
    "        compute_preprocess=compute_cluster,\n",
    "        compute_finetune=compute_cluster,\n",
    "        compute_model_evaluation=compute_cluster,\n",
    "        # map the dataset splits to parameters\n",
    "        train_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./samsum-dataset/small_train.jsonl\"\n",
    "        ),\n",
    "        validation_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./samsum-dataset/small_validation.jsonl\"\n",
    "        ),\n",
    "        test_file_path=Input(type=\"uri_file\", path=\"./samsum-dataset/small_test.jsonl\"),\n",
    "        evaluation_config=Input(\n",
    "            type=\"uri_file\", path=\"./samsum-dataset/text-generation-config.json\"\n",
    "        ),\n",
    "        # The following parameters map to the dataset fields\n",
    "        text_key=\"text\",\n",
    "        ground_truth_key=\"summary\",\n",
    "        # Training settings\n",
    "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
    "        **training_parameters,\n",
    "        **optimization_parameters\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": text_generation_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "\n",
    "# set continue on step failure to False\n",
    "pipeline_object.settings.continue_on_step_failure = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1727262272904
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading small_train.jsonl\u001b[32m (< 1 MB): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8.41M/8.41M [00:00<00:00, 57.1MB/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading small_validation.jsonl\u001b[32m (< 1 MB): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 460k/460k [00:00<00:00, 14.3MB/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n",
      "\u001b[32mUploading small_test.jsonl\u001b[32m (< 1 MB): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 474k/474k [00:00<00:00, 9.88MB/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.MLFlowModelJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: lime_eye_cg66rwxk95\n",
      "Web View: https://ml.azure.com/runs/lime_eye_cg66rwxk95?wsid=/subscriptions/e878de60-60e5-4a05-ba42-a9ab14136cc9/resourcegroups/ka-sand-rg/workspaces/ml-sandbox-core\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-10-08 15:26:55Z] Submitting 1 runs, first five are: cd21c6e8:a582c05d-b7af-4e73-866c-0826acc1a7ed\n"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Review training and evaluation metrics\n",
    "Viewing the job in AzureML studio is the best way to analyze logs, metrics and outputs of jobs. You can create custom charts and compare metics across different jobs. See https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics?tabs=interactive#view-jobsruns-information-in-the-studio to learn more. \n",
    "\n",
    "However, we may need to access and review metrics programmatically for which we will use MLflow, which is the recommended client for logging and querying metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job_name = \"your_pipeline_job_name\"\n",
    "\n",
    "# or\n",
    "\n",
    "pipeline_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1727261472143
    }
   },
   "outputs": [],
   "source": [
    "import mlflow, json\n",
    "\n",
    "mlflow_tracking_uri = workspace_ml_client.workspaces.get(\n",
    "    workspace_ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "# concat 'tags.mlflow.rootRunId=' and pipeline_job.name in single quotes as filter variable\n",
    "filter = \"tags.mlflow.rootRunId='\" + pipeline_job_name + \"'\"\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name], filter_string=filter, output_format=\"list\"\n",
    ")\n",
    "training_run = None\n",
    "evaluation_run = None\n",
    "# get the training and evaluation runs.\n",
    "# using a hacky way till 'Bug 2320997: not able to show eval metrics in FT notebooks - mlflow client now showing display names' is fixed\n",
    "for run in runs:\n",
    "    # check if run.data.metrics.epoch exists\n",
    "    if \"epoch\" in run.data.metrics:\n",
    "        training_run = run\n",
    "    # else, check if run.data.metrics.accuracy exists\n",
    "    elif \"rouge1\" in run.data.metrics:\n",
    "        evaluation_run = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1727261472152
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics:\n",
      "\n",
      "\n",
      "{\n",
      "  \"loss\": 1.7649,\n",
      "  \"grad_norm\": 5.759977340698242,\n",
      "  \"learning_rate\": 7.240474251063445e-09,\n",
      "  \"epoch\": 3.0,\n",
      "  \"eval_loss\": 1.89911949634552,\n",
      "  \"eval_runtime\": 108.3434,\n",
      "  \"eval_samples_per_second\": 7.55,\n",
      "  \"eval_steps_per_second\": 3.775,\n",
      "  \"checkpoint_save_step\": 22098.0,\n",
      "  \"train_runtime\": 18950.7583,\n",
      "  \"train_samples_per_second\": 2.332,\n",
      "  \"train_steps_per_second\": 1.166,\n",
      "  \"total_flos\": 3.5157706679936614e+17,\n",
      "  \"train_loss\": 1.860943827826315\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if training_run:\n",
    "    print(\"Training metrics:\\n\\n\")\n",
    "    print(json.dumps(training_run.data.metrics, indent=2))\n",
    "else:\n",
    "    print(\"No Training job found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1727261472157
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics:\n",
      "\n",
      "\n",
      "{\n",
      "  \"rouge1\": 0.5089949634090151,\n",
      "  \"rouge2\": 0.26415799641215976\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if evaluation_run:\n",
    "    print(\"Evaluation metrics:\\n\\n\")\n",
    "    print(json.dumps(evaluation_run.data.metrics, indent=2))\n",
    "else:\n",
    "    print(\"No Evaluation job found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Register the fine tuned model with the workspace\n",
    "\n",
    "We will register the model from the output of the fine tuning job. This will track lineage between the fine tuned model and the fine tuning job. The fine tuning job, further, tracks lineage to the foundation model, data and training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline job outputs:  {'trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f91d47d4680>}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# check if the `trained_model` output is available\n",
    "print(\"pipeline job outputs: \", workspace_ml_client.jobs.get(pipeline_job_name).outputs)\n",
    "\n",
    "# fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
    "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
    "    pipeline_job_name, \"trained_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = model_name + \"-samsum-textgen\"\n",
    "finetuned_model_name = finetuned_model_name.replace(\"/\", \"-\")\n",
    "print(\"path to register model: \", model_path_from_job)\n",
    "\n",
    "prepare_to_register_model = Model(\n",
    "    path=model_path_from_job,\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    name=finetuned_model_name,\n",
    "    version=2,  # use timestamp as version to avoid version conflict\n",
    "    description=model_name + \" fine tuned model for samsum textgen\",\n",
    ")\n",
    "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
    "\n",
    "# register the model from pipeline job output\n",
    "registered_model = workspace_ml_client.models.create_or_update(\n",
    "    prepare_to_register_model\n",
    ")\n",
    "print(\"registered model: \\n\", registered_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Deploy the fine tuned model to an online endpoint\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_name = \"Meta-Llama-3-8B-samsum-textgen\"\n",
    "timestamp = \"1728042057\"\n",
    "\n",
    "registered_model = workspace_ml_client.models.get(\n",
    "    name=finetuned_model_name, version=timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/subscriptions/e878de60-60e5-4a05-ba42-a9ab14136cc9/resourceGroups/ka-sand-rg/providers/Microsoft.MachineLearningServices/workspaces/ml-sandbox-core/models/Meta-Llama-3-8B-samsum-textgen/versions/1728042057'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_model.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/condav0/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [02:16<00:00,  5.95s/it]\n",
      "2024/10/07 17:40:37 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.15.0, required: mlflow==2.14.3)\n",
      " - jsonpickle (current: 3.2.2, required: jsonpickle==3.3.0)\n",
      " - mlflow-skinny (current: 2.15.0, required: mlflow-skinny==2.16.2)\n",
      " - azureml-core (current: 1.57.0.post3, required: azureml-core==1.57.0.post1)\n",
      " - azureml-metrics (current: 0.0.61, required: azureml-metrics[all]==0.0.60)\n",
      " - scikit-learn (current: 1.5.1, required: scikit-learn==1.5.2)\n",
      " - cryptography (current: 43.0.0, required: cryptography==43.0.1)\n",
      " - datasets (current: 3.0.1, required: datasets==2.17.1)\n",
      " - diffusers (current: 0.30.3, required: diffusers==0.26.3)\n",
      " - sentencepiece (current: 0.2.0, required: sentencepiece==0.1.99)\n",
      " - torch (current: 2.4.1, required: torch==2.2.2)\n",
      " - accelerate (current: 0.34.2, required: accelerate==0.33.0)\n",
      " - azureml-evaluate-mlflow (current: 0.0.61, required: azureml-evaluate-mlflow==0.0.60)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2024/10/07 17:40:37 WARNING mlflow.pyfunc: The version of Python that the model was saved in, `Python 3.10.14`, differs from the version of Python that is currently running, `Python 3.12.5`, and may be incompatible\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# We only input table and question, since system prompt is adeed in the prompt template.\n",
    "\n",
    "model_uri = f\"models:/{finetuned_model_name}/{timestamp}\"\n",
    "\n",
    "mlflow_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/07 17:40:47 WARNING azureml.evaluate.mlflow.hftransformers._task_based_predictors: preprocess script not found: No module named 'preprocess'\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "prediction = mlflow_model.predict(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"text\": [\n",
    "                \"Summarize this dialog:\\nHannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him 🙂\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n---\\nSummary:\\n\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hannah is looking for Betty's number. Amanda asks Larry, who has it. Hannah will text him.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1727261472172
    }
   },
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    OnlineRequestSettings,\n",
    ")\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "\n",
    "online_endpoint_name = \"samsum-textgen-\" + timestamp\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + registered_model.name\n",
    "    + \", fine tuned model for samsum textgen\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find here the list of SKU's supported for deployment - [Managed online endpoints SKU list](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_NC48ads_A100_v4\",  # use GPU instance type for faster explanations\n",
    "    instance_count=1,\n",
    "    # environment=environment,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        max_concurrent_requests_per_instance=1,\n",
    "        request_timeout_ms=90000,\n",
    "        max_queue_wait_ms=500,\n",
    "    ),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=49,\n",
    "        success_threshold=1,\n",
    "        timeout=299,\n",
    "        period=180,\n",
    "        initial_delay=180,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=10,\n",
    "        success_threshold=1,\n",
    "        timeout=10,\n",
    "        period=10,\n",
    "        initial_delay=2000,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint samsum-textgen-1728042057 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................"
     ]
    }
   ],
   "source": [
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Test the endpoint with sample data\n",
    "\n",
    "We will fetch some sample data from the test dataset and submit to online endpoint for inference. We will then show the display the scored labels alongside the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1727261472186
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize this dialog:\\nJack: Can you buy butt...</td>\n",
       "      <td>Thomas will buy butter. He is on his way home....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize this dialog:\\nJake: What are your pl...</td>\n",
       "      <td>Olivia has to sort out her accounts and upload...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Summarize this dialog:\\nJack: Can you buy butt...   \n",
       "1  Summarize this dialog:\\nJake: What are your pl...   \n",
       "\n",
       "                                             summary  \n",
       "0  Thomas will buy butter. He is on his way home....  \n",
       "1  Olivia has to sort out her accounts and upload...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read ./samsum-dataset/small_test.jsonl into a pandas dataframe\n",
    "test_df = pd.read_json(\"./samsum-dataset/small_test.jsonl\", lines=True)\n",
    "# take 5 random samples\n",
    "test_df = test_df.sample(n=2)\n",
    "# rebuild index\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "# rename the label_string column to ground_truth_label\n",
    "test_df = test_df.rename(columns={\"label_string\": \"ground_truth_label\"})\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1727261472192
    }
   },
   "outputs": [],
   "source": [
    "# create a json object with the key as \"input_data\" and value as a list of values from the text column of the test dataframe\n",
    "test_json = {\"input_data\": list(test_df[\"text\"])}\n",
    "# save the json object to a file named sample_score.json in the ./samsum-dataset folder\n",
    "with open(\"./samsum-dataset/sample_score.json\", \"w\") as f:\n",
    "    json.dump(test_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_endpoint_name = \"samsum-textgen-1728042057\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"./samsum-dataset/sample_score.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1727261472198
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54810/3589867497.py:2: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  response_df = pd.read_json(response)\n"
     ]
    }
   ],
   "source": [
    "# convert the response to a pandas dataframe and rename the label column as scored_label\n",
    "response_df = pd.read_json(response)\n",
    "response_df = response_df.rename(columns={0: \"scored_label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scored_label</th>\n",
       "      <th>extracted_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize this dialog:\\nJack: Can you buy butt...</td>\n",
       "      <td>Thomas is coming home. He will buy butter for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize this dialog:\\nJake: What are your pl...</td>\n",
       "      <td>Olivia needs to do her accounts and upload som...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        scored_label  \\\n",
       "0  Summarize this dialog:\\nJack: Can you buy butt...   \n",
       "1  Summarize this dialog:\\nJake: What are your pl...   \n",
       "\n",
       "                                   extracted_summary  \n",
       "0  Thomas is coming home. He will buy butter for ...  \n",
       "1  Olivia needs to do her accounts and upload som...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_df[\"extracted_summary\"] = [\n",
    "    s.split(\"Summary:\\n\")[1] for s in response_df[\"scored_label\"]\n",
    "]\n",
    "response_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1727261472204
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_summary</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jake has reserved 3 tickets for tomorrow at 7 ...</td>\n",
       "      <td>Jake reserved 3 tickets for tomorrow 7 pm. He ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kristina is watching the new season of America...</td>\n",
       "      <td>Kristina, Estefania and Jannette are watching ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   extracted_summary  \\\n",
       "0  Jake has reserved 3 tickets for tomorrow at 7 ...   \n",
       "1  Kristina is watching the new season of America...   \n",
       "\n",
       "                                             summary  \n",
       "0  Jake reserved 3 tickets for tomorrow 7 pm. He ...  \n",
       "1  Kristina, Estefania and Jannette are watching ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the test dataframe and the response dataframe on the index\n",
    "merged_df = pd.merge(test_df, response_df, left_index=True, right_index=True)\n",
    "merged_df[[\"extracted_summary\", \"summary\"]].head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Delete the online endpoint\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1727261472210
    }
   },
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "condav0"
  },
  "kernelspec": {
   "display_name": "condav0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
